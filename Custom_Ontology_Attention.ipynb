{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EOSjasccsFgi"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import pickle\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.preprocessing.text import Tokenizer\n",
        "from keras.models import Model\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.tokenize import word_tokenize\n",
        "from keras.layers import Input, Embedding, Conv1D, GlobalMaxPooling1D, Dense, Multiply, MaxPooling1D\n",
        "from tensorflow.keras.layers import Input, Dense, Embedding, Conv1D, GlobalMaxPooling1D, Dropout, Concatenate, Attention\n",
        "from gensim.models import Word2Vec\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "import nltk\n",
        "from sklearn.metrics import confusion_matrix,classification_report,accuracy_score\n",
        "from urllib.parse import quote\n",
        "from tensorflow.keras.models import load_model\n",
        "import requests\n",
        "import json\n",
        "import re"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EL6I7eatbbK",
        "outputId": "9b9f0d76-b392-48a0-8996-c50ca992f839"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gRa7rq56tWux"
      },
      "outputs": [],
      "source": [
        "def preprocess_text(text):\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    tokens = word_tokenize(text.lower())\n",
        "    tokens = [lemmatizer.lemmatize(token) for token in tokens if token.isalnum() and token not in stop_words]\n",
        "    return \" \".join(tokens)\n",
        "def extract_features(text, all_terms):\n",
        "    text = re.sub(r'\\W+', ' ', text.lower())\n",
        "    words = text.split()\n",
        "    features = [word for word in words if word in all_terms]\n",
        "    return features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "AyJHCbMotf3s",
        "outputId": "8421ed5c-4e9d-45f9-f35a-70eab2a634cf"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-c9a8cf2b-7459-448e-8852-6e361b803667\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>islam incompatible western culture quran pure ...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>group muslim</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>pick one religion angry islam seems good reaso...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>world muslim would honoured</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>problem islam country problem muslim problem p...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>329</th>\n",
              "      <td>evidence say</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>330</th>\n",
              "      <td>islam religion religion promotes action</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>331</th>\n",
              "      <td>thousand year muslim immigration country would...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>332</th>\n",
              "      <td>hard believe islam dangerous since muslim noth...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>333</th>\n",
              "      <td>many muslim lived country devoted violent people</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>334 rows Ã— 2 columns</p>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c9a8cf2b-7459-448e-8852-6e361b803667')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-c9a8cf2b-7459-448e-8852-6e361b803667 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-c9a8cf2b-7459-448e-8852-6e361b803667');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                                                  text  label\n",
              "0    islam incompatible western culture quran pure ...      0\n",
              "1                                         group muslim      1\n",
              "2    pick one religion angry islam seems good reaso...      1\n",
              "3                          world muslim would honoured      1\n",
              "4    problem islam country problem muslim problem p...      1\n",
              "..                                                 ...    ...\n",
              "329                                       evidence say      1\n",
              "330            islam religion religion promotes action      1\n",
              "331  thousand year muslim immigration country would...      1\n",
              "332  hard believe islam dangerous since muslim noth...      1\n",
              "333   many muslim lived country devoted violent people      1\n",
              "\n",
              "[334 rows x 2 columns]"
            ]
          },
          "execution_count": 38,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "train_df = pd.read_csv('/Data/train_data.csv')\n",
        "test_df=pd.read_csv('/Data/test_data.csv')\n",
        "train_df['label'] = train_df['label'].map({'hate': 0, 'nothate': 1})\n",
        "test_df['label'] = test_df['label'].map({'hate': 0, 'nothate': 1})\n",
        "train_df['text'] = train_df['text'].apply(preprocess_text)\n",
        "test_df['text'] = test_df['text'].apply(preprocess_text)\n",
        "test_df"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It opens and reads the contents of the file '/Ontologies/ontology.json' using the `json.load()` function, storing the data in the `ontology` variable.\n",
        "2. It assigns the variables `documents` and `documents2` to the data stored in `train_df` and `test_df`, respectively.\n",
        "3. It initializes empty lists `concepts`, `concepts2`, `values`, and `values2`.\n",
        "4. It iterates over each concept in the `ontology`.\n",
        "   - If the concept's `@id` key contains the specified URL ('http://www.semanticweb.org/lenovo/ontologies/2022/8/untitled-ontology-2#'), it extracts the concept name by splitting the `@id` value and appending it to the `concepts` list.\n",
        "   - If the concept has the key 'http://www.w3.org/2000/01/rdf-schema#label', it extracts the values associated with that key and appends them to the `values` list.\n",
        "5. It concatenates the `concepts` and `values` lists into `all_terms`.\n",
        "6. It concatenates the `concepts2` and `values2` lists into `all_terms2`.\n",
        "7. It adds a new column 'topics' to the `documents` DataFrame, where each value is the result of applying the `extract_features()` function to the 'text' column of `documents` and using `all_terms` as an argument.\n",
        "8. It adds a new column 'topics' to the `documents2` DataFrame, where each value is the result of applying the `extract_features()` function to the 'text' column of `documents2` and using `all_terms2` as an argument.\n",
        "9. It prints the `documents` DataFrame.\n",
        "10. It prints the `documents2` DataFrame.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2lNROz-Ys5a6",
        "outputId": "c0a82501-0adb-4638-cb22-734aa7aa71ea"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "                                                   text  label   topics\n",
            "0                    even say true muslim european want      0       []\n",
            "1                                        believe muzzie      0       []\n",
            "2             problem islam killing infidel must banned      0  [islam]\n",
            "3                        muslim human being human right      1       []\n",
            "4     think quite ridiculous though people seeking a...      1       []\n",
            "...                                                 ...    ...      ...\n",
            "2999  problem lack integration muslim population soc...      1       []\n",
            "3000  islam ideology opinion degenerated interpretat...      0  [islam]\n",
            "3001              sharia law perversion islam accept uk      0  [islam]\n",
            "3002  muslim want work live normal life go another c...      0       []\n",
            "3003             lost respect said muslim like pathetic      1       []\n",
            "\n",
            "[3004 rows x 3 columns]\n",
            "                                                  text  label topics\n",
            "0    islam incompatible western culture quran pure ...      0     []\n",
            "1                                         group muslim      1     []\n",
            "2    pick one religion angry islam seems good reaso...      1     []\n",
            "3                          world muslim would honoured      1     []\n",
            "4    problem islam country problem muslim problem p...      1     []\n",
            "..                                                 ...    ...    ...\n",
            "329                                       evidence say      1     []\n",
            "330            islam religion religion promotes action      1     []\n",
            "331  thousand year muslim immigration country would...      1     []\n",
            "332  hard believe islam dangerous since muslim noth...      1     []\n",
            "333   many muslim lived country devoted violent people      1     []\n",
            "\n",
            "[334 rows x 3 columns]\n"
          ]
        }
      ],
      "source": [
        "with open('/Ontologies/ontology.json', 'r') as f:\n",
        "    ontology = json.load(f)\n",
        "\n",
        "documents,documents2 = train_df,test_df\n",
        "concepts,concepts2 = [],[]\n",
        "values,values2 = [],[]\n",
        "\n",
        "for concept in ontology:\n",
        "    if '@id' in concept and 'http://www.semanticweb.org/lenovo/ontologies/2022/8/untitled-ontology-2#' in concept['@id']:\n",
        "        concepts.append(concept['@id'].split('#')[-1])\n",
        "        if 'http://www.w3.org/2000/01/rdf-schema#label' in concept:\n",
        "            values.extend([value['@value'] for value in concept['http://www.w3.org/2000/01/rdf-schema#label']])\n",
        "\n",
        "all_terms = concepts + values\n",
        "all_terms2 = concepts2 + values2\n",
        "documents['topics'] = documents['text'].apply(lambda x: extract_features(x, all_terms))\n",
        "documents2['topics'] = documents2['text'].apply(lambda x: extract_features(x, all_terms2))\n",
        "\n",
        "print(documents)\n",
        "print(documents2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jV4gs0yAtrbL"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.callbacks import ModelCheckpoint\n",
        "checkpoint_filepath = '/Models/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5'\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=checkpoint_filepath,\n",
        "    monitor='val_accuracy',\n",
        "    mode='max',\n",
        "    save_best_only=True,\n",
        "    verbose=1\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gPOno3Ftty-c"
      },
      "outputs": [],
      "source": [
        "MAX_SEQ_LENGTH = 256 # Maximum length of input sequence\n",
        "EMBEDDING_DIM = 50 # Dimensionality of word embeddings\n",
        "FILTER_SIZES = [2, 3, 4] # Filter sizes for convolutional layers\n",
        "NUM_FILTERS = 64 # Number of filters for each filter size\n",
        "DROPOUT_RATE = 0.5 # Dropout rate for regularization\n",
        "ATTENTION_UNITS = 128 # Number of units for attention layer\n",
        "NUM_CLASSES = 1 # Number of output classes\n",
        "LEARNING_RATE = 1e-3 # Learning rate for optimizer\n",
        "BATCH_SIZE = 32 # Batch size for training"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It defines the `GLOVE_FILE` variable with the path to a GloVe file ('/Models/glove.6B.50d.txt').\n",
        "2. It initializes an empty dictionary `embeddings_index` to store word embeddings.\n",
        "3. It opens the GloVe file using `open()` with UTF-8 encoding.\n",
        "4. It iterates over each line in the file.\n",
        "   - It splits each line into a list of values.\n",
        "   - It extracts the word from the first value in the list.\n",
        "   - It converts the remaining values into a NumPy array of float32 dtype, representing the word's embedding.\n",
        "   - It adds the word and its embedding to the `embeddings_index` dictionary.\n",
        "5. It initializes a `Tokenizer` object `tokenizer` with a limit of 5000 words.\n",
        "6. It fits the tokenizer on the 'text' column of the `train_df` DataFrame.\n",
        "7. It calculates the vocabulary size by adding 1 to the length of the tokenizer's word index.\n",
        "8. It initializes an embedding matrix `embedding_matrix` of zeros with dimensions (vocab_size, EMBEDDING_DIM), where EMBEDDING_DIM represents the dimensionality of the word embeddings (50 in this case).\n",
        "9. It iterates over each word and its corresponding index in the tokenizer's word index.\n",
        "   - It retrieves the embedding vector for the word from the `embeddings_index` dictionary.\n",
        "   - If the embedding vector exists, it assigns it to the corresponding row of the `embedding_matrix`.\n",
        "10. The code assumes the existence of the `train_df` DataFrame and the pre-trained GloVe file.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LO4mO0ZgtNsw"
      },
      "outputs": [],
      "source": [
        "GLOVE_FILE = '/Models/glove.6B.50d.txt'\n",
        "embeddings_index = {}\n",
        "with open(GLOVE_FILE, encoding='utf-8') as f:\n",
        "    for line in f:\n",
        "        values = line.split()\n",
        "        word = values[0]\n",
        "        coefs = np.asarray(values[1:], dtype='float32')\n",
        "        embeddings_index[word] = coefs\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(train_df['text'])\n",
        "vocab_size = len(tokenizer.word_index) + 1\n",
        "embedding_matrix = np.zeros((vocab_size, EMBEDDING_DIM))\n",
        "for word, i in tokenizer.word_index.items():\n",
        "    embedding_vector = embeddings_index.get(word)\n",
        "    if embedding_vector is not None:\n",
        "        embedding_matrix[i] = embedding_vector"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It defines an `Embedding` layer `embedding_layer` with the following parameters:\n",
        "   - `vocab_size`: The size of the vocabulary.\n",
        "   - `EMBEDDING_DIM`: The dimensionality of the word embeddings.\n",
        "   - `weights`: The pre-trained embedding matrix.\n",
        "   - `input_length`: The length of input sequences.\n",
        "   - `trainable`: Whether the embedding layer should be trainable or not. In this case, it is set to `False`.\n",
        "2. It creates a list `topics` by concatenating the 'topics' column values from `train_df` and `test_df`.\n",
        "3. It extracts unique topics from the `topics` list and assigns them to the `unique_topics` list.\n",
        "4. It creates a dictionary `topic_index` that maps each unique topic to its index.\n",
        "5. It extracts the 'topics' column values from `train_df` and assigns them to `train_topics`.\n",
        "6. It initializes an empty list `train_topic_vectors`.\n",
        "7. It iterates over each list of topics in `train_topics`.\n",
        "   - It initializes a topic vector `topic_vector` as an array of zeros with a length equal to the number of topics in `topic_index`.\n",
        "   - It iterates over each topic in the list of topics.\n",
        "     - It sets the corresponding index in `topic_vector` to 1, indicating the presence of that topic.\n",
        "   - It appends the `topic_vector` to the `train_topic_vectors` list.\n",
        "8. It extracts the 'topics' column values from `test_df` and assigns them to `test_topics`.\n",
        "9. It initializes an empty list `test_topic_vectors`.\n",
        "10. It iterates over each list of topics in `test_topics`.\n",
        "    - It initializes a topic vector `topic_vector` as an array of zeros with a length equal to the number of topics in `topic_index`.\n",
        "    - It iterates over each topic in the list of topics.\n",
        "      - It sets the corresponding index in `topic_vector` to 1, indicating the presence of that topic.\n",
        "    - It appends the `topic_vector` to the `test_topic_vectors` list.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gJO57hTWt1kj"
      },
      "outputs": [],
      "source": [
        "embedding_layer = tf.keras.layers.Embedding(vocab_size,\n",
        "                                            EMBEDDING_DIM,\n",
        "                                            weights=[embedding_matrix],\n",
        "                                            input_length=MAX_SEQ_LENGTH,\n",
        "                                            trainable=False)\n",
        "\n",
        "topics = train_df['topics'].tolist() + test_df['topics'].tolist()\n",
        "unique_topics = list(set([item for sublist in topics for item in sublist]))\n",
        "topic_index = {topic: i for i, topic in enumerate(unique_topics)}\n",
        "\n",
        "train_topics = train_df['topics'].tolist()\n",
        "train_topic_vectors = []\n",
        "for topics in train_topics:\n",
        "    topic_vector = np.zeros(len(topic_index))\n",
        "    for topic in topics:\n",
        "        topic_vector[topic_index[topic]] = 1\n",
        "    train_topic_vectors.append(topic_vector)\n",
        "\n",
        "test_topics = test_df['topics'].tolist()\n",
        "test_topic_vectors = []\n",
        "for topics in test_topics:\n",
        "    topic_vector = np.zeros(len(topic_index))\n",
        "    for topic in topics:\n",
        "        topic_vector[topic_index[topic]] = 1\n",
        "    test_topic_vectors.append(topic_vector)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It defines an `Input` layer `text_input` with the shape `(MAX_SEQ_LENGTH,)` and data type `'int32'`.\n",
        "2. It defines an `Input` layer `topic_input` with the shape `(len(topic_index),)` and data type `'float32'`.\n",
        "3. It applies the `embedding_layer` to the `text_input`, producing an `embedded_text` tensor.\n",
        "4. It initializes an empty list `conv_layers`.\n",
        "5. It iterates over each `filter_size` in the `FILTER_SIZES` list.\n",
        "   - It applies a `Conv1D` layer to the `embedded_text` tensor with parameters:\n",
        "     - `NUM_FILTERS`: The number of filters.\n",
        "     - `filter_size`: The size of the filters.\n",
        "     - `activation`: The activation function, set to `'relu'`.\n",
        "   - It applies a `GlobalMaxPooling1D` layer to the `conv_layer`, reducing the dimensionality.\n",
        "   - It appends the resulting `pool_layer` to the `conv_layers` list.\n",
        "6. It concatenates all the tensors in `conv_layers` along the channel axis, producing a merged tensor `merged_text`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NQt2LzlPvHpn"
      },
      "outputs": [],
      "source": [
        "text_input = tf.keras.layers.Input(shape=(MAX_SEQ_LENGTH,), dtype='int32')\n",
        "topic_input = tf.keras.layers.Input(shape=(len(topic_index),), dtype='float32')\n",
        "\n",
        "embedded_text = embedding_layer(text_input)\n",
        "conv_layers = []\n",
        "for filter_size in FILTER_SIZES:\n",
        "    conv_layer = tf.keras.layers.Conv1D(NUM_FILTERS,\n",
        "                                        filter_size,\n",
        "                                        activation='relu')(embedded_text)\n",
        "    pool_layer = tf.keras.layers.GlobalMaxPooling1D()(conv_layer)\n",
        "    conv_layers.append(pool_layer)\n",
        "merged_text = tf.keras.layers.concatenate(conv_layers)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It applies a `Dense` layer to the `topic_input` tensor with parameters:\n",
        "   - `ATTENTION_UNITS`: The number of units in the layer.\n",
        "   - `activation`: The activation function, set to `'tanh'`.\n",
        "   - The resulting tensor is assigned to `attention_layer`.\n",
        "2. It applies another `Dense` layer to the `attention_layer` tensor with parameters:\n",
        "   - `len(FILTER_SIZES)`: The number of filters, which represents the attention weights for each filter size.\n",
        "   - `activation`: The activation function, set to `'softmax'`.\n",
        "   - The resulting tensor is assigned to `attention_weights`.\n",
        "3. It reshapes the `attention_weights` tensor to have shape `(len(FILTER_SIZES), 1)` using `Reshape`.\n",
        "4. It performs element-wise multiplication between `merged_text` tensor and `attention_weights` tensor using `multiply`.\n",
        "5. It applies a `Lambda` layer to the resulting tensor, which sums the values along the second-to-last axis using `tf.keras.backend.sum`.\n",
        "6. It concatenates the `merged_text` tensor and the `topic_input` tensor along the channel axis using `concatenate`, producing `merged_inputs`.\n",
        "7. It applies a `Dense` layer to the `merged_inputs` tensor with parameters:\n",
        "   - `64`: The number of units in the layer.\n",
        "   - `activation`: The activation function, set to `'relu'`.\n",
        "   - The resulting tensor is assigned to `dense_layer`.\n",
        "8. It applies a `Dropout` layer to the `dense_layer` tensor with a dropout rate of `DROPOUT_RATE`.\n",
        "9. It applies another `Dense` layer to the `dropout_layer` tensor with parameters:\n",
        "   - `NUM_CLASSES`: The number of classes in the output layer.\n",
        "   - `activation`: The activation function, set to `'sigmoid'`.\n",
        "   - The resulting tensor is assigned to `output_layer`.\n",
        "10. It defines the model using `tf.keras.Model` with inputs `[text_input, topic_input]` and outputs `output_layer`.\n",
        "11. It compiles the model using parameters:\n",
        "    - `optimizer`: The optimizer, set to `'adam'`.\n",
        "    - `loss`: The loss function, set to `'binary_crossentropy'`.\n",
        "    - `metrics`: The evaluation metric, set to `['accuracy']`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uh6Hc3UIvJKu"
      },
      "outputs": [],
      "source": [
        "attention_layer = tf.keras.layers.Dense(ATTENTION_UNITS,activation='tanh')(topic_input)\n",
        "attention_weights = tf.keras.layers.Dense(len(FILTER_SIZES), activation='softmax')(attention_layer)\n",
        "attention_weights = tf.keras.layers.Reshape((len(FILTER_SIZES), 1))(attention_weights)\n",
        "merged_text = tf.keras.layers.multiply([merged_text, attention_weights])\n",
        "merged_text = tf.keras.layers.Lambda(lambda x: tf.keras.backend.sum(x, axis=-2))(merged_text)\n",
        "merged_inputs = tf.keras.layers.concatenate([merged_text, topic_input])\n",
        "dense_layer = tf.keras.layers.Dense(64, activation='relu')(merged_inputs)\n",
        "dropout_layer = tf.keras.layers.Dropout(DROPOUT_RATE)(dense_layer)\n",
        "output_layer = tf.keras.layers.Dense(NUM_CLASSES, activation='sigmoid')(dropout_layer)\n",
        "model = tf.keras.Model(inputs=[text_input, topic_input],outputs=output_layer)\n",
        "\n",
        "model.compile(optimizer='adam',loss='binary_crossentropy',metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vH-Wk_qKvLjm"
      },
      "outputs": [],
      "source": [
        "train_topic_vectors=np.asarray(train_topic_vectors)\n",
        "test_topic_vectors=np.asarray(test_topic_vectors)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "\n",
        "\n",
        "The below code snippet performs the following steps:\n",
        "\n",
        "1. It imports the `to_categorical` function from `keras.utils`.\n",
        "2. It splits the training data into training and validation sets using `train_test_split`.\n",
        "   - It assigns the 'text' column values of `train_df` to `X_train`.\n",
        "   - It assigns the 'label' column values of `train_df` to `Y_train`.\n",
        "   - It assigns the `train_topic_vectors` to `T_train`.\n",
        "   - It assigns the 'text' and 'label' column values of the validation set to `X_val` and `Y_val`, respectively.\n",
        "   - The test size is set to 0.1, and the random state is set to 42.\n",
        "3. It initializes a `Tokenizer` object `tokenizer` with a limit of 5000 words.\n",
        "4. It fits the tokenizer on the 'text' column values of `X_train`.\n",
        "5. It converts the text sequences in `X_train` to sequences of integers using `texts_to_sequences`.\n",
        "6. It pads the sequences in `X_train` with zeros or truncates them to a maximum length of `MAX_SEQ_LENGTH` using `pad_sequences`.\n",
        "7. It converts `Y_train` and `Y_val` to numpy arrays using the `values` attribute.\n",
        "8. It initializes another `Tokenizer` object `tokenizer` for the test data.\n",
        "9. It fits the tokenizer on the 'text' column values of `test_df`.\n",
        "10. It converts the text sequences in `test_df['text']` to sequences of integers using `texts_to_sequences`.\n",
        "11. It pads the sequences in `X_test` with zeros or truncates them to a maximum length of `MAX_SEQ_LENGTH` using `pad_sequences`.\n",
        "12. It assigns the 'label' column values of `test_df` to `Y_test`.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMBgxmDcvPVn"
      },
      "outputs": [],
      "source": [
        "from keras.utils import to_categorical\n",
        "\n",
        "X_train, X_val, Y_train, Y_val, T_train, T_val = train_test_split(train_df['text'], train_df['label'], train_topic_vectors, test_size=0.1, random_state=42)\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(X_train)\n",
        "X_train = tokenizer.texts_to_sequences(X_train)\n",
        "X_train = pad_sequences(X_train, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "X_val = tokenizer.texts_to_sequences(X_val)\n",
        "X_val = pad_sequences(X_val, maxlen=MAX_SEQ_LENGTH)\n",
        "Y_train = Y_train.values\n",
        "Y_val = Y_val.values\n",
        "\n",
        "tokenizer = tf.keras.preprocessing.text.Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(test_df['text'])\n",
        "X_test = tokenizer.texts_to_sequences(test_df['text'])\n",
        "X_test = pad_sequences(X_test, maxlen=MAX_SEQ_LENGTH)\n",
        "\n",
        "Y_test=test_df['label'].values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S4uLtuybvQ_m",
        "outputId": "b244a322-c5fe-44e3-ff4a-c1ed567176cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/20\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 0.6787 - accuracy: 0.5988\n",
            "Epoch 1: val_accuracy improved from -inf to 0.71429, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 5s 14ms/step - loss: 0.6747 - accuracy: 0.6019 - val_loss: 0.5869 - val_accuracy: 0.7143\n",
            "Epoch 2/20\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 0.5667 - accuracy: 0.7120\n",
            "Epoch 2: val_accuracy improved from 0.71429 to 0.73090, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.5686 - accuracy: 0.7096 - val_loss: 0.5277 - val_accuracy: 0.7309\n",
            "Epoch 3/20\n",
            "83/85 [============================>.] - ETA: 0s - loss: 0.4928 - accuracy: 0.7624\n",
            "Epoch 3: val_accuracy improved from 0.73090 to 0.78073, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.4922 - accuracy: 0.7632 - val_loss: 0.4880 - val_accuracy: 0.7807\n",
            "Epoch 4/20\n",
            "81/85 [===========================>..] - ETA: 0s - loss: 0.4321 - accuracy: 0.8052\n",
            "Epoch 4: val_accuracy did not improve from 0.78073\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.4284 - accuracy: 0.8084 - val_loss: 0.4632 - val_accuracy: 0.7575\n",
            "Epoch 5/20\n",
            "82/85 [===========================>..] - ETA: 0s - loss: 0.3638 - accuracy: 0.8518\n",
            "Epoch 5: val_accuracy improved from 0.78073 to 0.80066, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.3633 - accuracy: 0.8520 - val_loss: 0.4445 - val_accuracy: 0.8007\n",
            "Epoch 6/20\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 0.2849 - accuracy: 0.8910\n",
            "Epoch 6: val_accuracy did not improve from 0.80066\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.2892 - accuracy: 0.8901 - val_loss: 0.4927 - val_accuracy: 0.8007\n",
            "Epoch 7/20\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.2451 - accuracy: 0.9038\n",
            "Epoch 7: val_accuracy improved from 0.80066 to 0.80731, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.2406 - accuracy: 0.9079 - val_loss: 0.4443 - val_accuracy: 0.8073\n",
            "Epoch 8/20\n",
            "77/85 [==========================>...] - ETA: 0s - loss: 0.1831 - accuracy: 0.9359\n",
            "Epoch 8: val_accuracy improved from 0.80731 to 0.84053, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.1851 - accuracy: 0.9356 - val_loss: 0.4947 - val_accuracy: 0.8405\n",
            "Epoch 9/20\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 0.1401 - accuracy: 0.9533\n",
            "Epoch 9: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.1453 - accuracy: 0.9515 - val_loss: 0.4810 - val_accuracy: 0.8272\n",
            "Epoch 10/20\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 0.1185 - accuracy: 0.9645\n",
            "Epoch 10: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.1167 - accuracy: 0.9660 - val_loss: 0.5373 - val_accuracy: 0.8405\n",
            "Epoch 11/20\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.0907 - accuracy: 0.9704\n",
            "Epoch 11: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0916 - accuracy: 0.9697 - val_loss: 0.5385 - val_accuracy: 0.8239\n",
            "Epoch 12/20\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 0.0799 - accuracy: 0.9751\n",
            "Epoch 12: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0775 - accuracy: 0.9767 - val_loss: 0.5404 - val_accuracy: 0.8405\n",
            "Epoch 13/20\n",
            "81/85 [===========================>..] - ETA: 0s - loss: 0.0661 - accuracy: 0.9834\n",
            "Epoch 13: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0669 - accuracy: 0.9830 - val_loss: 0.6004 - val_accuracy: 0.8272\n",
            "Epoch 14/20\n",
            "80/85 [===========================>..] - ETA: 0s - loss: 0.0539 - accuracy: 0.9840\n",
            "Epoch 14: val_accuracy did not improve from 0.84053\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0541 - accuracy: 0.9837 - val_loss: 0.6461 - val_accuracy: 0.8405\n",
            "Epoch 15/20\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.0536 - accuracy: 0.9848\n",
            "Epoch 15: val_accuracy improved from 0.84053 to 0.85382, saving model to /content/drive/MyDrive/FYP-DATA-AND-RESOURCES/Models_and_data/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5\n",
            "85/85 [==============================] - 1s 7ms/step - loss: 0.0543 - accuracy: 0.9848 - val_loss: 0.7068 - val_accuracy: 0.8538\n",
            "Epoch 16/20\n",
            "79/85 [==========================>...] - ETA: 0s - loss: 0.0552 - accuracy: 0.9798\n",
            "Epoch 16: val_accuracy did not improve from 0.85382\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0540 - accuracy: 0.9808 - val_loss: 0.6726 - val_accuracy: 0.8472\n",
            "Epoch 17/20\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.0327 - accuracy: 0.9916\n",
            "Epoch 17: val_accuracy did not improve from 0.85382\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0315 - accuracy: 0.9922 - val_loss: 0.6988 - val_accuracy: 0.8372\n",
            "Epoch 18/20\n",
            "78/85 [==========================>...] - ETA: 0s - loss: 0.0283 - accuracy: 0.9940\n",
            "Epoch 18: val_accuracy did not improve from 0.85382\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0277 - accuracy: 0.9945 - val_loss: 0.7229 - val_accuracy: 0.8439\n",
            "Epoch 19/20\n",
            "81/85 [===========================>..] - ETA: 0s - loss: 0.0301 - accuracy: 0.9923\n",
            "Epoch 19: val_accuracy did not improve from 0.85382\n",
            "85/85 [==============================] - 1s 6ms/step - loss: 0.0300 - accuracy: 0.9926 - val_loss: 0.7221 - val_accuracy: 0.8272\n",
            "Epoch 20/20\n",
            "83/85 [============================>.] - ETA: 0s - loss: 0.0256 - accuracy: 0.9944\n",
            "Epoch 20: val_accuracy did not improve from 0.85382\n",
            "85/85 [==============================] - 1s 8ms/step - loss: 0.0257 - accuracy: 0.9937 - val_loss: 0.7663 - val_accuracy: 0.8106\n"
          ]
        }
      ],
      "source": [
        "history = model.fit([X_train, T_train],\n",
        "                    Y_train,\n",
        "                    epochs=20,\n",
        "                    batch_size=BATCH_SIZE,\n",
        "                    validation_data=([X_val, T_val], Y_val),\n",
        "                    callbacks=[checkpoint_callback])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HESlJHuRvduY"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.models import load_model\n",
        "saved_model=load_model('/Models/CustomOntologyAttetntion/Custom_Ontology_Attention_Model.h5')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cKRVrvDTvjio",
        "outputId": "cfe69dee-7fb0-4610-e998-4f416012ad14"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "11/11 [==============================] - 0s 8ms/step\n",
            "11/11 [==============================] - 0s 4ms/step - loss: 1.7134 - accuracy: 0.6078\n",
            "[[ 73 116]\n",
            " [ 15 130]]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.83      0.39      0.53       189\n",
            "           1       0.53      0.90      0.66       145\n",
            "\n",
            "    accuracy                           0.61       334\n",
            "   macro avg       0.68      0.64      0.60       334\n",
            "weighted avg       0.70      0.61      0.59       334\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pred=saved_model.predict([X_test,test_topic_vectors])\n",
        "loss,acc=saved_model.evaluate([X_test,test_topic_vectors],test_df['label'],batch_size=32)\n",
        "\n",
        "for i,x in enumerate(pred):#\"hate\":0,\"nothate\":1\n",
        "  if x >= 0.5:\n",
        "    pred[i] = 1\n",
        "  else:\n",
        "    pred[i] = 0\n",
        "cm = confusion_matrix(test_df['label'],pred)\n",
        "print(cm)\n",
        "cr = classification_report(test_df['label'],pred)\n",
        "print(cr)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
