{"cells":[{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"N14cYyidZeaH","outputId":"e921f865-fd4c-4dea-a129-1c94821ce0c6"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting anytree\n","  Downloading anytree-2.8.0-py2.py3-none-any.whl (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: six>=1.9.0 in /usr/local/lib/python3.10/dist-packages (from anytree) (1.16.0)\n","Installing collected packages: anytree\n","Successfully installed anytree-2.8.0\n"]}],"source":["!pip install anytree"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8b8D4pKfQCWx","outputId":"0069d8e1-3a4a-46a4-b693-f2c33a63cf2a"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/drive\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Unzipping corpora/stopwords.zip.\n","[nltk_data] Downloading package omw-1.4 to /root/nltk_data...\n","[nltk_data] Downloading package wordnet to /root/nltk_data...\n","[nltk_data] Downloading package punkt to /root/nltk_data...\n","[nltk_data]   Unzipping tokenizers/punkt.zip.\n","[nltk_data] Error loading nltk: Package 'nltk' not found in index\n","[nltk_data] Downloading package averaged_perceptron_tagger to\n","[nltk_data]     /root/nltk_data...\n","[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"]}],"source":["import json\n","import anytree\n","from anytree import AnyNode,RenderTree\n","import pandas as pd\n","from sklearn.naive_bayes import MultinomialNB,BernoulliNB,GaussianNB\n","from sklearn.ensemble import VotingClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.linear_model import LogisticRegression\n","import string\n","import re\n","import nltk\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","nltk.download('wordnet')\n","nltk.download('punkt')\n","nltk.download('nltk')\n","nltk.download('averaged_perceptron_tagger')\n","from nltk.corpus import stopwords\n","from nltk.stem.porter import PorterStemmer\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.metrics import accuracy_score,confusion_matrix,classification_report\n","import numpy as np\n","from xgboost import XGBClassifier"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ie5TFVFMXVRK"},"source":["# **Jaccard Similarity using Custom Ontology**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fShQ08QHZPQM"},"outputs":[],"source":["file=open(\"/Ontologies/ontology.json\")\n","data=json.load(file)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","The below code snippet defines two functions: `get_synonyms` and `createTree`.\n","\n","### Function `get_synonyms(data,component)`\n","- Parameters:\n","  - `data`: The input data, expected to be a list.\n","  - `component`: The component to search for in the data.\n","- Returns: The synonyms corresponding to the given component in the data.\n","\n","The function performs the following steps:\n","1. It iterates over each item in the data.\n","2. If the second element of the item matches the given component, it returns the third element (synonyms).\n","3. If no match is found, it returns `None`.\n","\n","### Function `createTree(data)`\n","- Parameters:\n","  - `data`: The input data, expected to be a list.\n","- Returns: The root node of the created tree.\n","\n","The function performs the following steps:\n","1. It initializes a set `parentsList` to store unique parent values.\n","2. It initializes an empty dictionary `dictionaryTree` to store the tree structure.\n","3. It iterates over each item in the data.\n","   - It adds the first element of the item to the `parentsList` set.\n","4. It iterates over each parent in the `parentsList`.\n","   - It initializes an empty list as the value for the parent in the `dictionaryTree`.\n","   - It iterates over each value in the data.\n","     - If the first element of the value matches the current parent, it appends the second element to the list of the parent's values in the `dictionaryTree`.\n","5. It creates a root node with `id=\"islam\"`, empty `synonymList`, level `0`, and empty `parentId`.\n","6. It initializes a list `currentList` with `\"islam\"`.\n","7. It enters a while loop until the `dictionaryTree` is empty.\n","   - It initializes an empty list `tempList`.\n","   - It iterates over each current in the `currentList`.\n","     - If the current exists as a key in the `dictionaryTree`:\n","       - It iterates over each child in the values of the current in the `dictionaryTree`.\n","         - It appends the child to the `tempList`.\n","         - It searches for the parent node with id equal to the current in the existing tree using `anytree.search.find`.\n","         - It creates a new node with the child as the id, the found parent as the parent, synonyms obtained from `get_synonyms`, level incremented by 1, and the current as the parentId.\n","         - It updates the `tempCurrent` variable with the current.\n","       - It removes the current key from the `dictionaryTree`.\n","   - It clears the `currentList` and copies the elements from the `tempList`.\n","8. It returns the root node of the created tree.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RQvbbGSDZPQN"},"outputs":[],"source":["def get_synonyms(data,component):\n","    for item in data:\n","        if item[1]==component:\n","            return item[2]\n","def createTree(data):\n","    parentsList=set()\n","    dictionaryTree=dict()\n","    for item in data:\n","        parentsList.add(item[0])\n","    \n","    for item in parentsList:\n","        dictionaryTree[item]=[]\n","        for value in data:\n","            if value[0]==item:\n","                dictionaryTree[item].append(value[1])\n","  \n","    currentList=['islam']\n","    tempCurrent=None\n","    parent=AnyNode(id=\"islam\",synonymList=[],level=0,parentId=\"\")\n","    while len(dictionaryTree)>0:\n","        tempList=[]\n","        for current in currentList:\n","            if current in dictionaryTree.keys():\n","                for child in dictionaryTree[current]:\n","                    tempList.append(child)\n","                    tempNode=anytree.search.find(parent,lambda node:node.id==current)\n","                    node=AnyNode(id=child,parent=tempNode,synonymList=get_synonyms(data,child),level=(tempNode.level)+1,parentId=current)\n","                    tempCurrent=current\n","                del dictionaryTree[current]\n","        currentList.clear()\n","        currentList=tempList.copy()\n","    return parent"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","The below code snippet performs the following steps:\n","\n","1. It initializes an empty list `finalData` to store the extracted ontology data.\n","2. It iterates over each item in the `data`.\n","   - It checks if the string \"http://www.semanticweb.org/lenovo/ontologies/2022/8/untitled-ontology-2\" is present in the '@id' key of the item.\n","   - It checks if the string \"http://www.w3.org/2002/07/owl#Class\" is present in the '@type' key of the item.\n","   - It checks if the key 'http://www.w3.org/2000/01/rdf-schema#subClassOf' is present in the item.\n","   - If all the conditions are satisfied, it extracts the component and parent values from the item and creates a list of synonyms.\n","   - It appends the extracted data as a list [parent, component, synonymList] to the `finalData` list.\n","3. It calls the `createTree` function with the `finalData` list to create the main ontology tree, assigning the root node to `MainTreeOntology`.\n","4. It iterates over the nodes in the ontology tree using `RenderTree` from the `anytree` library.\n","   - It prints the node's id with appropriate indentation based on the tree structure.\n","\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmEhFjf_ZPQN","outputId":"4ba2b700-faa4-477f-abf5-897cdd3f6e83"},"outputs":[{"name":"stdout","output_type":"stream","text":["islam\n","└── pillars\n","    ├── component\n","    │   ├── ablution_component\n","    │   │   ├── masah_of_head\n","    │   │   ├── masah_of_neck\n","    │   │   ├── rinse_mouth\n","    │   │   ├── wash_face\n","    │   │   ├── wash_feet_till_ankle\n","    │   │   ├── wash_hands\n","    │   │   ├── wash_hands_till_elbow\n","    │   │   └── wash_nostrils\n","    │   ├── ghusal_component\n","    │   │   ├── clean_body\n","    │   │   ├── clean_whole_body\n","    │   │   ├── rinse_nostrils\n","    │   │   ├── rinse_the_mouth\n","    │   │   └── wudu\n","    │   ├── kiblah\n","    │   ├── prayer_component\n","    │   │   ├── bowing\n","    │   │   ├── fatiha\n","    │   │   ├── intention\n","    │   │   ├── prostration\n","    │   │   ├── qadah\n","    │   │   ├── qauma\n","    │   │   ├── recitation\n","    │   │   ├── salaam\n","    │   │   ├── sitting\n","    │   │   ├── standing\n","    │   │   ├── standing_after_bow\n","    │   │   └── takbir\n","    │   └── tayammum_component\n","    │       ├── wipe_face\n","    │       └── wipe_hands\n","    ├── fasting\n","    │   ├── farz_fasting\n","    │   │   └── ramazan\n","    │   ├── nafil_fasting\n","    │   │   ├── random\n","    │   │   ├── shabban\n","    │   │   └── shawwal\n","    │   ├── sunnat_fasting\n","    │   │   ├── arafat\n","    │   │   ├── brightmoon\n","    │   │   └── muharram\n","    │   └── wajib_fasting\n","    │       ├── kaffara\n","    │       └── nazar\n","    ├── hajj\n","    ├── namaz\n","    │   ├── farz_namaz\n","    │   │   ├── asar\n","    │   │   ├── fajar\n","    │   │   ├── isha\n","    │   │   ├── maghrib\n","    │   │   └── zuhar\n","    │   ├── nafil_namaz\n","    │   │   ├── awabeen\n","    │   │   ├── chasht\n","    │   │   ├── haajat\n","    │   │   ├── ishraq\n","    │   │   ├── istikhara\n","    │   │   ├── khusoof\n","    │   │   ├── kusoof\n","    │   │   └── tahiyyatul_ul_wuzu\n","    │   ├── sunnat_namaz\n","    │   │   ├── funeral\n","    │   │   ├── istisqa\n","    │   │   ├── tahajud\n","    │   │   ├── tarawwih\n","    │   │   ├── tasbih\n","    │   │   └── travel\n","    │   └── wajib_namaz\n","    │       ├── eid\n","    │       ├── friday\n","    │       ├── salat_ul_witr\n","    │       └── tawaff\n","    ├── shahadah\n","    │   ├── allah\n","    │   └── last_prophet\n","    └── zakat\n","        ├── zakat_ul_fitr\n","        └── zakat_wealth\n"]}],"source":["finalData=[]\n","for item in data:\n","    if \"http://www.semanticweb.org/lenovo/ontologies/2022/8/untitled-ontology-2\" in str(item['@id']):\n","        if \"http://www.w3.org/2002/07/owl#Class\" in str(item['@type'][0]):\n","            if \"http://www.w3.org/2000/01/rdf-schema#subClassOf\" in item.keys():\n","                componentArray=str.split(str(item['@id']),'#')\n","                component=componentArray[1]\n","                parentArray=str.split(str(item['http://www.w3.org/2000/01/rdf-schema#subClassOf'][0]),'#')\n","                parent=parentArray[1]\n","                parent=parent[:len(parent)-2]\n","                if 'http://www.w3.org/2000/01/rdf-schema#label' in item.keys():\n","                   synonymList=[]\n","                   for synonym in item['http://www.w3.org/2000/01/rdf-schema#label']:\n","                        synonymList.append(str(synonym['@value']))\n","                # appendToFile(parent+\":\"+component+\":Synonyms=\"+str(synonymList))  \n","                finalData.append([parent,component,synonymList]) \n","                synonymList=[]\n","                \n","\n","MainTreeOntology=createTree(finalData)\n","for pre, fill, node in RenderTree(MainTreeOntology):\n","    print(\"%s%s\" % (pre, node.id))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iSVF42dMZPQQ"},"outputs":[],"source":["def lemmatizer(text):\n","    wordnet_lemmatizer = WordNetLemmatizer()\n","    lemm_text = [wordnet_lemmatizer.lemmatize(word) for word in text]\n","    return lemm_text\n","def preprocess_text(df):\n","  df['text'] = df['text'].str.lower()\n","  df['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n","  df['text'] = df['text'].apply(lambda x: x.split())\n","  stop_words = set(stopwords.words('english'))\n","  df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n","  df['text']=df['text'].apply(lambda x:lemmatizer(x))\n","  df['text'] = df['text'].apply(lambda x: ' '.join(x))\n","  return df['text']\n","\n","def preprocess_text2(df):\n","  df['text'] = df['text'].str.lower()\n","  df['text'] = df['text'].apply(lambda x: re.sub('[^a-zA-Z]', ' ', x))\n","  df['text'] = df['text'].apply(lambda x: x.split())\n","  stop_words = set(stopwords.words('english'))\n","  df['text'] = df['text'].apply(lambda x: [word for word in x if word not in stop_words])\n","  stemmer = PorterStemmer()\n","  df['text'] = df['text'].apply(lambda x: [stemmer.stem(word) for word in x])\n","  df['text'] = df['text'].apply(lambda x: ' '.join(x))\n","  return df['text']"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vrzF_jxTJ1r6"},"outputs":[],"source":["df = pd.read_csv(\"/Data/train_data.csv\")\n","df['label'] = df['label'].replace({\"hate\":0,\"nothate\":1})"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"diIrGUPKZPQQ"},"outputs":[],"source":["df['text'] = preprocess_text(df)\n","sentences = []\n","\n","for value in df['text']:\n","  value = str(value)\n","  sentences.append(value.strip(\"[]'\"))\n","i = 0\n","entities = []\n","word = \"\"\n","for x in range(len(df['text'])):\n","  temp = nltk.word_tokenize(sentences[x])\n","  text = nltk.pos_tag(temp)\n","  for bigram in text:\n","      word = word + \" \" + bigram[0]\n","  entities.append(word)\n","  word = \"\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["\n","\n","The below code snippet defines several functions: `isEmpty`, `getPillar`, and `getClassificationJaccard`.\n","\n","### Function `isEmpty(dictionary)`\n","- Parameter:\n","  - `dictionary`: The input dictionary to check for emptiness.\n","- Returns: A boolean indicating whether the dictionary is empty.\n","\n","The function performs the following steps:\n","1. It initializes a boolean variable `empty` as `True`.\n","2. It iterates over each key in the dictionary.\n","   - If any key has a value greater than 0, it updates `empty` to `False` and immediately returns `empty`.\n","3. If the loop completes without finding any non-zero values, it returns `empty`.\n","\n","### Function `getPillar(nodeId)`\n","- Parameter:\n","  - `nodeId`: The id of the node to find its corresponding pillar.\n","- Returns: The id of the pillar node for the given nodeId.\n","\n","The function performs the following steps:\n","1. It searches for the node with id equal to `nodeId` in the `MainTreeOntology`.\n","2. It initializes a variable `parent` with the found node.\n","3. It iterates from 0 to `nodeToFind.level-2`.\n","   - It updates `parent` to its parent node in each iteration.\n","4. It returns the id of the parent node.\n","\n","### Function `getClassificationJaccard(toCheck, counter)`\n","- Parameters:\n","  - `toCheck`: The list of words to check for classification.\n","  - `counter`: A counter value.\n","- Returns: The label of the pillar node based on the classification Jaccard score.\n","\n","The function performs the following steps:\n","1. It initializes an empty dictionary `document_words` to store the counts of words from `toCheck`.\n","2. It iterates over each word in `toCheck` and initializes its count in `document_words` to 0.\n","3. It iterates over each word in `toCheck`.\n","   - It searches for the word in the `MainTreeOntology`.\n","     - If found, it increments the count of the word in `document_words`.\n","     - If not found, it searches for the word in the synonyms of nodes in the `MainTreeOntology`.\n","       - If found, it adds the word as a key in `document_words` and increments its count.\n","4. It removes the keys from `document_words` with a count of 0 and removes the keys `'islam'` and `'pillars'` if present.\n","5. It calculates the `totalLevels` as the maximum level of nodes in the `MainTreeOntology`.\n","6. It initializes an empty list `connections` to store the connections between nodes in `document_words`.\n","7. While `document_words` is not empty:\n","   - It iterates over the levels from 0 to `totalLevels`.\n","     - It finds all nodes in the `MainTreeOntology` with a level equal to the current level.\n","     - For each node, if the node's id is present in `document_words` and its count is greater than 0:\n","       - It adds the node's id and level to `connections`.\n","       - It decrements the count of the node in `document_words`.\n","8. It initializes an empty dictionary `fragments` to store the fragments of nodes and their connections.\n","9. It iterates over the connections and populates `fragments` based on the level of the connection.\n","10. It iterates over the connections again and populates `fragments` with connections to the pillars.\n","11. It checks if the pillars are present in `fragments`, and if not,it adds them to fragments with empty connection lists.\n","12. It opens a file with a filename based on the counter value.\n","13. It writes the connections in fragments to the file.\n","14. It calculates the scores for each fragment based on the number of connections and the union of words in toCheck.\n","15. It initializes variables max and label to track the maximum score and the corresponding label.\n","16. It compares each fragment's score to the current maximum and updates max and label accordingly.\n","17. It returns the label as the final classification label based on the highest score.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"wWGZwgitZPQR"},"outputs":[],"source":["def isEmpty(dictionary):\n","    empty=True\n","    for key in dictionary.keys():\n","        if dictionary[key]>0:\n","            empty=False\n","            return empty\n","    return empty\n","def getPillar(nodeId):\n","    nodeToFind=anytree.search.find(MainTreeOntology,lambda node:nodeId==node.id)\n","    parent=nodeToFind\n","    for i in range(0,nodeToFind.level-2):\n","        parent=parent.parent\n","    return parent.id\n","def getClassificationJaccard(toCheck,counter):\n","    document_words=dict()\n","    for word in toCheck:\n","        document_words[word]=0\n","\n","    for word in toCheck:\n","        tempNode=anytree.search.find(MainTreeOntology,lambda node:node.id==word)\n","        if tempNode is not None:\n","            document_words[word]+=1\n","        else:\n","            tempNode=anytree.search.find(MainTreeOntology,lambda node:word in node.synonymList)\n","            if tempNode is not None:\n","                if tempNode.id not in document_words.keys():\n","                    document_words[tempNode.id]=1\n","                else:\n","                    document_words[tempNode.id]+=1\n","    toDelete=[]\n","    for key in document_words.keys():\n","        if document_words[key]==0:\n","            toDelete.append(key)\n","    for key in toDelete:\n","        del document_words[key]\n","    if \"islam\" in document_words.keys():\n","        del document_words['islam']\n","    if \"pillars\" in document_words.keys():\n","        del document_words['pillars']\n","    totalLevels=0\n","    for pre, fill, node in RenderTree(MainTreeOntology):\n","        if node.level>totalLevels:\n","            totalLevels=node.level\n","    totalLevels\n","    connections=[]\n","    while not isEmpty(document_words):\n","        for i in range(0,totalLevels+1):\n","            nodes=anytree.search.findall(MainTreeOntology,filter_=lambda node: node.level==i)\n","            for node in nodes:\n","                if node.id in document_words.keys():\n","                    if document_words[node.id]>0:\n","                        connections.append([node.id,node.level])\n","                        document_words[node.id]-=1\n","    fragments=dict()\n","    for item in connections:\n","        if item[1]==2:\n","            if item[0] in fragments.keys():\n","                fragments[item[0]][0]+=1\n","            else:\n","                fragments[item[0]]=[1,[]]\n","    for item in connections:\n","        if item[1]>2:\n","            pillar=getPillar(item[0])\n","            if pillar in fragments.keys():\n","                fragments[pillar][1].append(item)\n","            else:\n","                fragments[pillar]=[0,[]]\n","                fragments[pillar][1].append(item)\n","\n","    pillars=[\"fasting\",\"namaz\",\"shahadah\",\"hajj\",\"component\",\"zakat\"]\n","    for pillar in pillars:\n","        if pillar not in fragments.keys():\n","            fragments[pillar]=[0,[]]\n","    filename=\"r_nr_ontological_features\\\\\"+str(counter)\n","    file=open('%s.txt' % filename,\"w\",encoding='utf-8')\n","    for fragment in fragments.keys():\n","        if len(fragments[fragment][1])>0:\n","            for word in fragments[fragment][1]:\n","\n","                file.write(str(word[0])+\"\\n\")\n","    scores=dict()\n","    # print(fragments)\n","    for key in fragments.keys():\n","        scores[key]=fragments[key][0]+len(fragments[key][1])\n","    union=len(toCheck)\n","    for keys in scores.keys():\n","        scores[keys]=scores[keys]/union\n","    max=0\n","    label=\"\"\n","    for keys in scores.keys():\n","        if scores[keys]>max:\n","            max=scores[keys]\n","            label=keys\n","    return label"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"YXNJAqP9ZPQS","outputId":"54e872e5-411d-4302-bc0c-ba33690e6b1c"},"outputs":[{"name":"stdout","output_type":"stream","text":["Accuracy: 0.4437416777629827\n","[[  54 1571]\n"," [ 100 1279]]\n","              precision    recall  f1-score   support\n","\n","           0       0.35      0.03      0.06      1625\n","           1       0.45      0.93      0.60      1379\n","\n","    accuracy                           0.44      3004\n","   macro avg       0.40      0.48      0.33      3004\n","weighted avg       0.40      0.44      0.31      3004\n","\n"]}],"source":["bow = []\n","for x in entities:\n","  bow.append(x.strip().split(\" \"))\n","listOfLabels=[]\n","i=1\n","for item in bow:\n","    label=getClassificationJaccard(item,i)\n","    i+=1\n","    listOfLabels.append(label)\n","    \n","for i in range(0,len(listOfLabels)):#\"hate\":0,\"nothate\":1\n","    if listOfLabels[i] == \"\":\n","      listOfLabels[i] = 1\n","    else:\n","      listOfLabels[i] = 0\n","\n","print(\"Accuracy: \"+str(accuracy_score(df['label'],listOfLabels)))\n","cm = confusion_matrix(df['label'],listOfLabels)\n","print(cm)\n","print(classification_report(df['label'],listOfLabels))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"tagqe1XLT4wN"},"source":["# **MultinomialNB & Logistic Regression**"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hQGyZCocZPQU"},"outputs":[],"source":["df = pd.read_csv(\"/Data/train_data.csv\")\n","df['label'] = df['label'].replace({\"hate\":0,\"nothate\":1})\n","df['text'] = preprocess_text2(df)\n","\n","df2 = pd.read_csv(\"/Data/test_data.csv\")\n","df2['label'] = df2['label'].replace({\"hate\":0,\"nothate\":1})\n","df2['text'] = preprocess_text2(df2)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"R_GXM5deZPQU","outputId":"8adb71e0-9efc-4bcc-ba4c-d567b30e402a"},"outputs":[{"name":"stdout","output_type":"stream","text":["['abb' 'abhor' 'abhorr' ... 'zoo' 'zulpan' 'zwemer']\n","  (0, 2507)\t0.4006738904780003\n","  (0, 770)\t0.4580337503288457\n","  (0, 1526)\t0.16581950382312524\n","  (0, 2377)\t0.4538587923756214\n","  (0, 2006)\t0.39473454552860404\n","  (0, 772)\t0.49027160263514946\n","(3004, 2618)\n"]}],"source":["from sklearn.feature_extraction.text import TfidfVectorizer\n","vec = TfidfVectorizer()\n","data_tfidf = vec.fit_transform(df['text'])\n","print(vec.get_feature_names_out())\n","print(data_tfidf[0])\n","print(data_tfidf.shape)\n","test_data = vec.transform(df2['text'])"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nmDfiQP_ZPQV","outputId":"29973ee0-9562-4cc1-ef70-ca2a9a8b0e48"},"outputs":[{"name":"stdout","output_type":"stream","text":["Original Logistic Regression Score Training: 0.9101198402130493\n","Original Logistic Regression Score Testing: 0.8622754491017964\n","[[166  23]\n"," [ 23 122]]\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.88      0.88       189\n","           1       0.84      0.84      0.84       145\n","\n","    accuracy                           0.86       334\n","   macro avg       0.86      0.86      0.86       334\n","weighted avg       0.86      0.86      0.86       334\n","\n","\n","Original MultinomialNB Score Training: 0.9067909454061251\n","Original MultinomialNB Score Testing: 0.874251497005988\n","[[171  18]\n"," [ 24 121]]\n","              precision    recall  f1-score   support\n","\n","           0       0.88      0.90      0.89       189\n","           1       0.87      0.83      0.85       145\n","\n","    accuracy                           0.87       334\n","   macro avg       0.87      0.87      0.87       334\n","weighted avg       0.87      0.87      0.87       334\n","\n","\n"]}],"source":["# \"hate\":0,\"nothate\":1\n","lr=LogisticRegression(penalty='l2',max_iter=1000,C=1,random_state=36,class_weight={1: 1.12, 0:1},solver=\"lbfgs\")\n","lr_bow=lr.fit(data_tfidf,df['label'])\n","print(\"Original Logistic Regression Score Training:\",lr.score(data_tfidf,df['label']))\n","print(\"Original Logistic Regression Score Testing:\",lr.score(test_data,df2['label']))\n","\n","print(confusion_matrix(df2['label'],lr_bow.predict(test_data)))\n","print(classification_report(df2['label'],lr_bow.predict(test_data)),end='\\n\\n')\n","\n","gn2 = MultinomialNB(class_prior=[1.15,0.9])\n","gn2_bow=gn2.fit(data_tfidf,df['label'])\n","\n","print(\"Original MultinomialNB Score Training:\",gn2.score(data_tfidf,df['label']))\n","print(\"Original MultinomialNB Score Testing:\",gn2.score(test_data,df2['label']))\n","\n","print(confusion_matrix(df2['label'],gn2_bow.predict(test_data)))\n","print(classification_report(df2['label'],gn2_bow.predict(test_data)),end='\\n\\n')"]}],"metadata":{"accelerator":"GPU","colab":{"provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.2"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"f2ccb58c476f33ba3e3aee7ac07234ef6b8217ef24ad64d2a7d4fed1a57c1cd2"}}},"nbformat":4,"nbformat_minor":0}
